{% load doc_links %}

<div class="section">
  <p>The Perl environment in ScraperWiki comes with the Scraperwiki module loaded.</p>

  <p>The source code implementation of these functions can be found
  <a href="https://bitbucket.org/ScraperWiki/scraperwiki/src/FIXME/scraperlibs/perl/lib/Scraperwiki.pm">here</a>.</p>
</div>

<h3>Scraping</h3>
<p>You can also use any Perl HTTP library.</p>

<dl>
<dt>Scraperwiki::<strong>scrape</strong>(url[, params])</dt>
    <dd>Returns the downloaded string from the given url.</dd>
    <dd>params are send as a POST if set.
    </dd>
</dl>

<h3><span id="sql"></span>Datastore (SQLite)</h3>
<p>ScraperWiki provides a fully-fledged SQLite database for each scraper which
you can save to.  You can read the data back that has been committed by other
scrapers, or extract it through the API.
</p>
<p>
{% doc_link_full 'LANG_datastore_guide' language %} for examples.
<a href="http://www.sqlite.org/lang.html">SQL as understood by SQLite</a> for the query language.
</p>

<dl>
<dt>Scraperwiki::<strong>save_sqlite</strong>(unique_keys, data[, table_name="swdata", verbose=2])</dt>
    <dd>Saves a data record into the datastore into the table given by table_name.  </dd>
    <dd>data is a hash with string or symbol field names as keys, unique_keys is an array
        that is a subset of data.keys which determines when a record is to be
        over-written.
    <dd>For large numbers of records data can be a list of hashes.
    <dd>verbose alters what is shown in the Data tab of the editor.
    </dd>

<dt>Scraperwiki::<strong>attach</strong>(name[, asname])</dt>
    <dd>Attaches to the datastore of another scraper of name name.</dd>
    <dd>asname is an optional alias for the attached datastore.
    </dd>

<dt>Scraperwiki::<strong>select</strong>(val1[, val2])</dt>
    <dd>Executes a select command on the datastore, e.g. select("* from swdata limit 10")</dd>
    <dd>Returns an array of hashes for the records that have been selected.</dd>
    <dd>val2 is an optional array of parameters when the select command contains '?'s.
    </dd>


<dt>Scraperwiki::<strong>sqliteexecute</strong>(val1[, val2])</dt>
    <dd>Executes any arbitrary sqlite command (except attach), e.g. create, delete, insert or drop.</dd>
    <dd>val2 is an optional array of parameters if the command in val1 contains question marks.</dd>
    <dd>(e.g. "insert into swdata values (?,?,?)").
    </dd>

<dt>Scraperwiki::<strong>commit</strong>()</dt>
    <dd>Commits to the file after a series of execute commands.  (save_sqlite() auto-commits after every action).
    </dd>

<dt>Scraperwiki::<strong>show_tables</strong>([dbname])</dt>
    <dd>Returns an array of tables and their schemas in either the current or an attached database.</dd>

<dt>Scraperwiki::<strong>table_info</strong>(name)</dt>
    <dd>Returns an array of attributes for each element of the table.</dd>

<dt>Scraperwiki::<strong>save_var</strong>(key, value)</dt>
    <dd>Saves an arbitrary single-value into a sqlite table called "swvariables".
        e.g. Can be used to make scrapers able to continue after an interruption.
    </dd>

<dt>Scraperwiki::<strong>get_var</strong>(key[, default])</dt>
    <dd>Retrieves a single value that was saved by save_var.
    </dd>

</dl>

<h3>Views</h3>

<dt>Scraperwiki::<strong>httpresponseheader</strong>(headerkey, headervalue)</dt>
    <dd>Set the content-type header to something other than HTML when using a ScraperWiki "view"</dd>
    <dd>(e.g. "Content-Type", "image/png")
    </dd>


<h3>Geocoding</h3>

<dl>

<dt>Scraperwiki::<strong>gb_postcode_to_latlng</strong>(postcode)</dt>
    <dd>Returns an array [lat, lng] in WGS84 coordinates representing the central point of a UK postcode area.
    </dd>

</dl>
