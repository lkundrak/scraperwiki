{% load doc_links %}

<div class="section">
  <p>The Ruby environment in ScraperWiki comes with the ScraperWiki module loaded.</p>

  <p>The source code implementation of these functions can be found 
  <a href="https://bitbucket.org/ScraperWiki/scraperwiki/src/efd21ea1c875/scraperlibs/scraperwiki.rb">here</a>.</p>
</div>

<h3>Scraping</h3>
<p>You can also use any Ruby HTTP library.</p>

<dl>
<dt>ScraperWiki.<strong>scrape</strong>(url[, params])</dt>
    <dd>Returns the downloaded string from the given url.</dd>
    <dd>params are send as a POST if set.
    </dd>
</dl>

<h3><span id="sql"></span>Datastore (SQLite)</h3>
<p>ScraperWiki provides a fully-fledged SQLite database for each scraper which
you can save to.  You can read the data back that has been committed by other
scrapers, or extract it through the API. 
</p>
<p>
{% doc_link_full 'LANG_datastore_guide' language %} for examples.
<a href="http://www.sqlite.org/lang.html">SQL as understood by SQLite</a> for the query language.
</p>

<dl>
<dt>ScraperWiki.<strong>save_sqlite</strong>(unique_keys, data[, table_name="swdata", verbose=2])</dt>
    <dd>Saves a data record into the datastore into the table given by table_name.  </dd>
    <dd>data is a hash with string or symbol field names as keys, unique_keys is an array
        that is a subset of data.keys which determines when a record is to be
        over-written.
    <dd>For large numbers of records data can be a list of hashes.
    <dd>verbose alters what is shown in the Data tab of the editor.
    </dd>

<dt>ScraperWiki.<strong>attach</strong>(name[, asname])</dt>
    <dd>Attaches to the datastore of another scraper of name name.</dd>
    <dd>asname is an optional alias for the attached datastore.
    </dd>

<dt>ScraperWiki.<strong>select</strong>(val1[, val2])</dt>
    <dd>Executes a select command on the datastore, e.g. select("* from swdata limit 10")</dd>
    <dd>Returns an array of hashes for the records that have been selected.</dd>
    <dd>val2 is an optional array of parameters when the select command contains '?'s.
    </dd>


<dt>ScraperWiki.<strong>sqliteexecute</strong>(val1[, val2])</dt>
    <dd>Executes any arbitrary sqlite command (except attach), e.g. create, delete, insert or drop.</dd>
    <dd>val2 is an optional array of parameters if the command in val1 contains question marks.</dd>
    <dd>(e.g. "insert into swdata values (?,?,?)").
    </dd>

<dt>ScraperWiki.<strong>commit</strong>()</dt>
    <dd>Commits to the file after a series of execute commands.  (save_sqlite() auto-commits after every action).
    </dd>

<dt>ScraperWiki.<strong>show_tables</strong>([dbname])</dt>
    <dd>Returns an array of tables and their schemas in either the current or an attached database.</dd>

<dt>ScraperWiki.<strong>table_info</strong>(name)</dt>
    <dd>Returns an array of attributes for each element of the table.</dd>

<dt>ScraperWiki.<strong>save_var</strong>(key, value)</dt>
    <dd>Saves an arbitrary single-value into a sqlite table called "swvariables". 
        e.g. Can be used to make scrapers able to continue after an interruption.
    </dd>

<dt>ScraperWiki.<strong>get_var</strong>(key[, default])</dt>
    <dd>Retrieves a single value that was saved by save_var.
    </dd>

</dl>

<h3>Views</h3>

<dt>ScraperWiki.<strong>httpresponseheader</strong>(headerkey, headervalue)</dt>
    <dd>Set the content-type header to something other than HTML when using a ScraperWiki "view"</dd>
    <dd>(e.g. "Content-Type", "image/png")
    </dd>


<h3>Geocoding</h3>

<dl>

<dt>ScraperWiki.<strong>gb_postcode_to_latlng</strong>(postcode)</dt>
    <dd>Returns an array [lat, lng] in WGS84 coordinates representing the central point of a UK postcode area.
    </dd>

</dl>


