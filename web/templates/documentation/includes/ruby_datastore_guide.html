<p>Every scraper comes with its own <a href="http://www.sqlite.org/lang.html">SQLite database</a> 
which it can store data to. You can also read data from other scrapers.</p>

<h2>Saving data, basic</h2>

<p>Easy save function for most use.</p>
<code>ScraperWiki::save_sqlite(unique_keys=["a"], data={"a"=>1, "bbb"=>"Hi there"})
</code>

<p>You can use symbols for keys if you prefer (new LXC sandbox only).</p>
<code>ScraperWiki::save_sqlite(unique_keys=[:a], data={:a=>1, :bbb=>"Hi there"})
</code>

<p>If the values for the unique_keys matches a record already there, it will over-write.</p>
<code>ScraperWiki::save_sqlite(unique_keys=["a"], data={"a"=>1, "bbb"=>"Bye there"})
</code>

<p>You can add new columns into the database and the table will extend automatically.
(The print is so you can see the comment.)</p>
<code>require 'pp'
pp ScraperWiki::save_sqlite(unique_keys=["a"], data={"a"=>2, "bbb"=>"Going home", "cccnew"=>-999.9})
</code>

<h2>Saving data, advanced</h2>

<p>Each new column is given an <a href="http://www.sqlite.org/datatype3.html#affinity">affinity</a> 
according to the type of the value it is first given (text, integer, real).  It is okay to save a string 
in a column that was defined as an integer, but it will sometimes be converted if possible.
You can define a column with no affinity by giving it the name ending in "_blob".</p>
<code>ScraperWiki::save_sqlite(unique_keys=["a"], data={"a"=>1, "dddd_blob"=>"999.999"})
pp ScraperWiki::save_sqlite("swdata")
</code>

<p>Further parameters in the save function are table_name (the default table name is "swdata"), 
and verbose (which doesn't send messages to the data tab if set to 0</p>
<code>ScraperWiki::save_sqlite(unique_keys, data, table_name="swdata", verbose=2)
</code>

<p>You can also list a list of dicts in the save for greater speed</p>
<code>data = { {"a"=>10}, {"a"=>20}, {"a"=>30} ]
ScraperWiki::save_sqlite(["a"], data)
</code>

<h2>Saving data, variables</h2>

<p>It's often useful to be able to quickly and easily save one metadata
value. For example, to record which page the last run of the scraper managed to
get up to. 

<code>ScraperWiki::save_var('last_page', 27)
puts ScraperWiki::get_var('last_page')
</code>

<p>It's stored in a simple table called swvariables.</p>

<h2>Finding out the schema</h2>

<p>To see the dict of table_names mapping to schemas.</p>
<code>pp ScraperWiki::show_tables()
</code>

<p>Info about a particular table (and its elements) can be queried.</p>
<code>info = ScraperWiki::table_info(name="swdata")
for column in info
    puts column.name +" "+ column.type
end
</code>

<h2>Direct SQL for saving</h2>

<p>You can execute direct SQL commands.  Back-ticks ` are used to quote column names that are have spaces in them.</p>
<code>ScraperWiki::sqliteexecute("create table ttt (xx int, `yy` string)")
ScraperWiki::sqliteexecute("insert into ttt values (?,?)", [9, 'hello'])
ScraperWiki::sqliteexecute("insert or replace into ttt values (:xx, :yy)", {"xx"=>10, "yy"=>"again"})
</code>

<p>Don't forget after doing your inserts you need to commit the result.  (The save() command always automatically commits.)</p>
<code>ScraperWiki::commit()
</code>

<h2>Direct SQL for selecting</h2>

<p>Selection can be done by execution of a select function.</p>
<code>pp ScraperWiki::sqliteexecute("select * from ttt")
pp ScraperWiki::sqliteexecute("select min(xx), max(xx) from ttt group by yy")
</code>

<p>The result will be a dict with a list for keys, and a list of rows (which are lists) for the 
corresponding values.</p>
<code>{ "keys"=> ["xx", "yy"], data=>[[9, 'hello'], [10, 'again']] }
</code>

<p>The shorthand select command gives the results in dicts.</p>
<code>print scraperwiki.sqlite.select("* from ttt")
[{'yy'=> 'hello', 'xx'=> 9}, {'yy'=>'again', 'xx'=>10}]
</code>

<h2>Direct SQL for modifying schemas</h2>

<p>You can also clean up by deleting rows or dropping tables</p>
<code>ScraperWiki::sqliteexecute("delete from ttt where xx=9")
ScraperWiki::sqliteexecute("drop table if exists ttt")
ScraperWiki::commit()
</code>

<p>There's also a "clear datastore" button on the scraper page, which is useful
for starting again during development if the schema is in a mess.</p>

<p>If you like, you can completely ignore the ScraperWiki save command,
and construct all your schemas explicitly.</p>

<h2>Reading data from other scrapers</h2>

<p>To access data from other scrapers we attach to them, using their shortname (the name in the URL).</p>
<code>ScraperWiki::attach("new_americ_foundation_drone_strikes")
pp ScraperWiki::select("* from new_americ_foundation_drone_strikes.swdata limit 2")
</code>

<p>To make it easy, you can change the name of the database you import it as.</p>
<code>ScraperWiki::attach("new_americ_foundation_drone_strikes", "src")
pp ScraperWiki::table_info("src.swdata")
</code>

<p>Access to other scrapers data through the attach interface is read-only.</p>
