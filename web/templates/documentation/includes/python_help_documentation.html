{% load doc_links %}

<div class="section">
  <p>In addition to all the standard Python libraries for downloading and parsing pages from the web, 
  ScraperWiki provides the following built in library.</p>
  
  <p>You can access it by including this at the top of your code:</p>

  <code>import scraperwiki</code>

  <p>The source code implementation of these functions can be found 
  <a href="https://bitbucket.org/ScraperWiki/scraperwiki/src/efd21ea1c875/scraperlibs/">here</a>.</p>
</div>


<h3>Scraping</h3>
<p>You can also use any Python HTTP library.</p>

<dl>
<dt>scraperwiki.<strong>scrape</strong>(url[, params])</dt>
    <dd>Returns the downloaded string from the given url.</dd>
    <dd>params are send as a POST if set.
    </dd>
</dl>


<h3><span id="sql"></span>Datastore (SQLite)</h3>
<p>ScraperWiki provides a fully-fledged SQLite database for each scraper which
you can save to.  You can read the data back that has been committed by other
scrapers, or extract it through the API. 
</p>
<p>
{% doc_link_full 'LANG_datastore_guide' language %} for examples.
<a href="http://www.sqlite.org/lang.html">SQL as understood by SQLite</a> for the query language.
</p>


<dl>
<dt>scraperwiki.<strong>sqlite.save</strong>(unique_keys, data[, table_name="swdata", verbose=2])</dt>
    <dd>Saves a data record into the datastore into the table given by table_name.  <br/>
    <dd>data is a dict object with field names as keys, unique_keys is a subset
        of data.keys() which determines when a record is over-written.
    <dd>For large numbers of records data can be a list of dicts.
    <dd>$verbose alters what is shown in the Data tab of the editor.
    </dd>

<dt>scraperwiki.<strong>sqlite.attach</strong>(name[, asname])</dt>
    <dd>Attaches to the datastore of another scraper with the given name.</dd>
    <dd>asname is an optional alias for the attached datastore.
    </dd>

<dt>scraperwiki.<strong>sqlite.select</strong>(val1[, val2], verbose=1)</dt>
    <dd>Executes a select command on the datastore, e.g. select("* from swdata limit 10")</dd>
    <dd>Returns a list of dicts that have been selected.</dd>
    <dd>val2 is an optional list of parameters if the select command contains '?'s.
    </dd>

<dt>scraperwiki.<strong>sqlite.execute</strong>(val1[, val2, verbose=1])</dt>
    <dd>Executes any arbitrary sqlite command (except attach), e.g. create, delete, insert or drop.</dd>
    <dd>val2 is an optional list of parameters if the select command contains question marks.
    <dd>(e.g. "insert into swdata values (?,?,?)").
    </dd>

<dt>scraperwiki.<strong>sqlite.commit</strong>()</dt>
    <dd>Commits to the file after a series of execute commands.  (sqlite.save auto-commits after every action).
    </dd>

<dt>scraperwiki.<strong>sqlite.show_tables</strong>([dbname])</dt>
    <dd>Returns an array of tables and their schemas in either the current or an attached database.</dd>

<dt>scraperwiki.<strong>sqlite.table_info</strong>(name)</dt>
    <dd>Returns an array of attributes for each element of the table.</dd>

<dt>scraperwiki.<strong>sqlite.save_var</strong>(key, value)</dt>
    <dd>Saves an arbitrary single-value into a table called "swvariables". 
        e.g. Can be used to make scrapers able to continue after an interruption.
    </dd>

<dt>scraperwiki.<strong>sqlite.get_var</strong>(key[, default])</dt>
    <dd>Retrieves a single value that was saved by save_var.
        Only works for string, float, or int types.  
        For anything else, use the pickle library to turn it into a string.
    </dd>
</dl>

<dt>scraperwiki.<strong>sqlite.SqliteError</strong></dt>
    <dd>An exception that is raised when there is, for example, a syntax error in your sql query.
    </dd>
</dl>


<h3>Views</h3>

<dt>scraperwiki.<strong>utils.httpresponseheader</strong>(headerkey, headervalue)</dt>
    <dd>Set the content-type header to something other than HTML when using a ScraperWiki "view" 
    <dd>(e.g. "Content-Type", "image/png")
    </dd>

<dt>scraperwiki.<strong>dumpMessage</strong>({"content":base64.encodestring(binstring), "message_type":"console", "encoding":"base64"})</dt>
    <dd>The method for outputting the binary string binstring that contains, for example, a png image</dd>

<dt>scraperwiki.<strong>utils.GET</strong>()</dt>
    <dd>The query_string map when using a ScraperWiki "view".</dd>
    <dd>prefer to use: dict(cgi.parse_qsl(os.getenv("QUERY_STRING", ""))).</dd>

<h3>Geocoding</h3>
<p>Some installed functions to help you transform between different coordinate systems.</p> 

<dl>
<dt>scraperwiki.<strong>geo.os_easting_northing_to_latlng</strong>(easting, northing[, grid='GB'])</dt>
    <dd>Converts a <a href="http://en.wikipedia.org/wiki/British_national_grid_reference_system">OSGB</a> 
        or <a href="http://en.wikipedia.org/wiki/Irish_grid_reference_system">OSIE</a> (grid='IE') 
        grid reference to a WGS84 (lat, lng) pair.
    </dd>

<dt>scraperwiki.<strong>geo.extract_gb_postcode</strong>(string)</dt>
    <dd>Attempts to extract a UK postcode from a given string.
    </dd>

<dt>scraperwiki.<strong>geo.gb_postcode_to_latlng</strong>(postcode)</dt>
    <dd>Returns a WGS84 (lat, lng) pair for the central location of a UK postcode.
    </dd>


</dl>


<h3>Miscellaneous</h3>
<dl>

<dt>scraperwiki.<strong>pdftoxml</strong>(pdfdata)</dt>
    <dd>Convert a byte string containing a PDF file into an XML file containing the coordinates 
        and font of each text string.  <br/>
        Refer to 
        <a href="https://scraperwiki.com/scrapers/new/python?template=advanced-scraping-pdfs">the example</a> 
        for more details.
    </dd>

<dt>modulename = scraperwiki.<strong>utils.swimport</strong>(name)</dt>
    <dd>Imports the code from another scraper as the module "modulename".
    </dd>

</dl>

